{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10k Sentiment Analysis Project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "# from pandas import json_normalize\n",
    "# from datetime import date\n",
    "# from dateutil.relativedelta import relativedelta\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "# import json\n",
    "# import urllib.request\n",
    "# import xml.etree.ElementTree as ET\n",
    "# from collections import Counter\n",
    "\n",
    "# Import ML libraries:\n",
    "# import numpy as np\n",
    "# from nltk import ngrams\n",
    "# from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "# nltk.download('vader_lexicon')       # Uncomment this to download the latest package when you run for the first time\n",
    "\n",
    "# Import plotting libraries:\n",
    "# import panel as pn\n",
    "# import holoviews as hv\n",
    "# import hvplot.pandas\n",
    "# from holoviews import opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_lookup = {\n",
    "    'INTC': '0000050863'}#,\n",
    "#     'GTLS': '0000892553'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Helper Functions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "\n",
    "class SecAPI(object):\n",
    "    SEC_CALL_LIMIT = {'calls': 10, 'seconds': 1}\n",
    "\n",
    "    @staticmethod\n",
    "    @sleep_and_retry\n",
    "    # Dividing the call limit by half to avoid coming close to the limit\n",
    "    @limits(calls=SEC_CALL_LIMIT['calls'] / 2, period=SEC_CALL_LIMIT['seconds'])\n",
    "    def _call_sec(url):\n",
    "        return requests.get(url)\n",
    "\n",
    "    def get(self, url):\n",
    "        return self._call_sec(url).text\n",
    "\n",
    "\n",
    "def print_ten_k_data(ten_k_data, fields, field_length_limit=50):\n",
    "    indentation = '  '\n",
    "\n",
    "    print('[')\n",
    "    for ten_k in ten_k_data:\n",
    "        print_statement = '{}{{'.format(indentation)\n",
    "        for field in fields:\n",
    "            value = str(ten_k[field])\n",
    "\n",
    "            # Show return lines in output\n",
    "            if isinstance(value, str):\n",
    "                value_str = '\\'{}\\''.format(value.replace('\\n', '\\\\n'))\n",
    "            else:\n",
    "                value_str = str(value)\n",
    "\n",
    "            # Cut off the string if it gets too long\n",
    "            if len(value_str) > field_length_limit:\n",
    "                value_str = value_str[:field_length_limit] + '...'\n",
    "\n",
    "            print_statement += '\\n{}{}: {}'.format(indentation * 2, field, value_str)\n",
    "\n",
    "        print_statement += '},'\n",
    "        print(print_statement)\n",
    "    print(']')\n",
    "\n",
    "\n",
    "def plot_similarities(similarities_list, dates, title, labels):\n",
    "    assert len(similarities_list) == len(labels)\n",
    "\n",
    "    plt.figure(1, figsize=(10, 7))\n",
    "    for similarities, label in zip(similarities_list, labels):\n",
    "        plt.title(title)\n",
    "        plt.plot(dates, similarities, label=label)\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://www.sec.gov/Archives/edgar/data/50863/000005086321000010/0000050863-21-000010-index.htm',\n",
      "  '10-K',\n",
      "  '2021-01-22'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/50863/000005086320000011/0000050863-20-000011-index.htm',\n",
      "  '10-K',\n",
      "  '2020-01-24'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/50863/000005086319000007/0000050863-19-000007-index.htm',\n",
      "  '10-K',\n",
      "  '2019-02-01'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/50863/000005086318000007/0000050863-18-000007-index.htm',\n",
      "  '10-K',\n",
      "  '2018-02-16'),\n",
      " ('https://www.sec.gov/Archives/edgar/data/50863/000005086317000012/0000050863-17-000012-index.htm',\n",
      "  '10-K',\n",
      "  '2017-02-17')]\n"
     ]
    }
   ],
   "source": [
    "sec_api = SecAPI()\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_sec_data(cik, doc_type, start=0, count=60):\n",
    "    rss_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
    "        '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n",
    "        .format(cik, doc_type, start, count)\n",
    "    sec_data = sec_api.get(rss_url)\n",
    "    feed = BeautifulSoup(sec_data.encode('utf-8'), 'xml').feed\n",
    "    entries = [\n",
    "        (\n",
    "            entry.content.find('filing-href').getText(),\n",
    "            entry.content.find('filing-type').getText(),\n",
    "            entry.content.find('filing-date').getText())\n",
    "        for entry in feed.find_all('entry', recursive=False)]\n",
    "    return entries\n",
    "\n",
    "example_ticker = 'INTC'\n",
    "sec_data = {}\n",
    "\n",
    "for ticker, cik in cik_lookup.items():\n",
    "    sec_data[ticker] = get_sec_data(cik, '10-K')\n",
    "    \n",
    "pprint.pprint(sec_data[example_ticker][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading INTC Fillings: 100%|██████████████████████████████████████████████████| 29/29 [02:42<00:00,  5.59s/filling]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Document:\n",
      "\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\">\n",
      "<head>\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n",
      "<title>SEC.gov | Request Rate Threshold Exceeded</title>\n",
      "<style>\n",
      "html {height: 100%}\n",
      "body {height: 100%; margin:0; padding:0;}\n",
      "#header {background-color:#003968; color:#fff; padding:15px 20px 10px 20px;font-family:Arial, Helvetica, sans-serif; font-size:20px; border-bottom:solid 5px #000;}\n",
      "#footer {background-color:#003968; color:#fff; padding:15px 20px;font-family:Arial, Helvetica, sans-serif; font-size:20px;}\n",
      "#content {max-width:650px;margin:60px auto; padding:0 20px 100px 20px; background-image:url(seal_bw.png);background-repeat:no-repeat;background-position:50% 100%;}\n",
      "h1 {font-family:Georgia, Times, serif; font-size:20px;}\n",
      "h2 {text-align:center; font-family:Georgia, Times, serif; font-size:20px; width:100%; border-bottom:solid #999 1px;padding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_fillings_by_ticker = {}\n",
    "\n",
    "for ticker, data in sec_data.items():\n",
    "    raw_fillings_by_ticker[ticker] = {}\n",
    "    for index_url, file_type, file_date in tqdm(data, desc='Downloading {} Fillings'.format(ticker), unit='filling'):\n",
    "        if (file_type == '10-K'):\n",
    "            file_url = index_url.replace('-index.htm', '.txt').replace('.txtl', '.txt')            \n",
    "            \n",
    "            raw_fillings_by_ticker[ticker][file_date] = sec_api.get(file_url)\n",
    "            \n",
    "print('Example Document:\\n\\n{}...'.format(next(iter(raw_fillings_by_ticker[example_ticker].values()))[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting Documents from INTC Fillings: 100%|███████████████████████████████████████| 28/28 [00:02<00:00, 11.59filling/s]\n",
      "Getting Documents from GTLS Fillings: 100%|███████████████████████████████████████| 23/23 [00:05<00:00,  3.88filling/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 Filed on 2021-01-22:\n",
      "\n",
      "<TYPE>10-K\n",
      "<SEQUENCE>1\n",
      "<FILENAME>intc-20201226.htm\n",
      "<DESCRIPTION>10-K\n",
      "<TEXT>\n",
      "<XBRL>\n",
      "<?xml version=\"1.0\" ?><!--XBRL Document Created with Wdesk from Workiva--><!--Copyright 2021 Workiva--><!--r:ed8a096...\n",
      "\n",
      "Document 1 Filed on 2021-01-22:\n",
      "\n",
      "<TYPE>EX-21.1\n",
      "<SEQUENCE>2\n",
      "<FILENAME>a12262020q4-ex211.htm\n",
      "<DESCRIPTION>EX-21.1\n",
      "<TEXT>\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\"><html><head...\n",
      "\n",
      "Document 2 Filed on 2021-01-22:\n",
      "\n",
      "<TYPE>EX-23.1\n",
      "<SEQUENCE>3\n",
      "<FILENAME>a12262020q4-ex231.htm\n",
      "<DESCRIPTION>EX-23.1\n",
      "<TEXT>\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\"><html><head...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "\n",
    "\n",
    "def get_documents(text):\n",
    "    extracted_docs = []\n",
    "    \n",
    "    doc_start_pattern = re.compile(r'<DOCUMENT>')\n",
    "    doc_end_pattern = re.compile(r'</DOCUMENT>')   \n",
    "    \n",
    "    doc_start_is = [x.end() for x in      doc_start_pattern.finditer(text)]\n",
    "    doc_end_is = [x.start() for x in doc_end_pattern.finditer(text)]\n",
    "    \n",
    "    for doc_start_i, doc_end_i in zip(doc_start_is, doc_end_is):\n",
    "            extracted_docs.append(text[doc_start_i:doc_end_i])\n",
    "    \n",
    "    return extracted_docs\n",
    "\n",
    "\n",
    "filling_documents_by_ticker = {}\n",
    "\n",
    "for ticker, raw_fillings in raw_fillings_by_ticker.items():\n",
    "    filling_documents_by_ticker[ticker] = {}\n",
    "    for file_date, filling in tqdm(raw_fillings.items(), desc='Getting Documents from {} Filings'.format(ticker), unit='filing'):\n",
    "        filling_documents_by_ticker[ticker][file_date] = get_documents(filling)\n",
    "        \n",
    "print('\\n\\n'.join([\n",
    "    'Document {} Filed on {}:\\n{}...'.format(doc_i, file_date, doc[:200])\n",
    "    for file_date, docs in filling_documents_by_ticker[example_ticker].items()\n",
    "    for doc_i, doc in enumerate(docs)][:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_type(doc):\n",
    "    \n",
    "    type_pattern = re.compile(r'<TYPE>[^\\n]+')\n",
    "    \n",
    "    doc_type = type_pattern.findall(doc)[0][len('<TYPE>'):] \n",
    "    \n",
    "    return doc_type.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    cik: '0000050863'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>intc-2020122...\n",
      "    file_date: '2021-01-22'},\n",
      "  {\n",
      "    cik: '0000050863'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>a12282019q4-...\n",
      "    file_date: '2020-01-24'},\n",
      "  {\n",
      "    cik: '0000050863'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>2\\n<FILENAME>a12282019q41...\n",
      "    file_date: '2020-01-24'},\n",
      "  {\n",
      "    cik: '0000050863'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>a12292018q4-...\n",
      "    file_date: '2019-02-01'},\n",
      "  {\n",
      "    cik: '0000050863'\n",
      "    file: '\\n<TYPE>10-K\\n<SEQUENCE>1\\n<FILENAME>a12302017q4-...\n",
      "    file_date: '2018-02-16'},\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "ten_ks_by_ticker = {}\n",
    "\n",
    "for ticker, filling_documents in filling_documents_by_ticker.items():\n",
    "    ten_ks_by_ticker[ticker] = []\n",
    "    for file_date, documents in filling_documents.items():\n",
    "        for document in documents:\n",
    "            if get_document_type(document) == '10-k':\n",
    "                ten_ks_by_ticker[ticker].append({\n",
    "                    'cik': cik_lookup[ticker],\n",
    "                    'file': document,\n",
    "                    'file_date': file_date})\n",
    "                \n",
    "print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['cik', 'file', 'file_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = remove_html_tags(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning INTC 10-Ks: 100%|███████████████████████████████████████████████████████████| 25/25 [06:55<00:00, 16.60s/10-K]\n",
      "Cleaning GTLS 10-Ks: 100%|███████████████████████████████████████████████████████████| 19/19 [04:33<00:00, 14.38s/10-K]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    file_clean: '\\n10-k\\n1\\nintc-20201226.htm\\n10-k\\n\\n\\n\\nintc-20...},\n",
      "  {\n",
      "    file_clean: '\\n10-k\\n1\\na12282019q4-10kdocument.htm\\n10-k\\n\\n\\...},\n",
      "  {\n",
      "    file_clean: '\\n10-k\\n2\\na12282019q410kdocumentcourte.pdf\\n10-k...},\n",
      "  {\n",
      "    file_clean: '\\n10-k\\n1\\na12292018q4-10kdocument.htm\\n10-k\\n\\n\\...},\n",
      "  {\n",
      "    file_clean: '\\n10-k\\n1\\na12302017q4-10kdocument.htm\\n10-k\\n\\n\\...},\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    for ten_k in tqdm(ten_ks, desc='Cleaning {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "        ten_k['file_clean'] = clean_text(ten_k['file'])\n",
    "print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatize INTC 10-Ks: 100%|██████████████████████████████████████████████████████████| 25/25 [03:46<00:00,  9.06s/10-K]\n",
      "Lemmatize GTLS 10-Ks: 100%|██████████████████████████████████████████████████████████| 19/19 [00:35<00:00,  1.89s/10-K]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    file_lemma: '['10', 'k', '1', 'intc', '20201226', 'htm', '10',...},\n",
      "  {\n",
      "    file_lemma: '['10', 'k', '1', 'a12282019q4', '10kdocument', 'h...},\n",
      "  {\n",
      "    file_lemma: '['10', 'k', '2', 'a12282019q410kdocumentcourte', ...},\n",
      "  {\n",
      "    file_lemma: '['10', 'k', '1', 'a12292018q4', '10kdocument', 'h...},\n",
      "  {\n",
      "    file_lemma: '['10', 'k', '1', 'a12302017q4', '10kdocument', 'h...},\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def lemmatize_words(words):\n",
    "\n",
    "    lemmatized_words = [WordNetLemmatizer().lemmatize(word, 'v') for word in words]\n",
    "    \n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "word_pattern = re.compile('\\w+')\n",
    "\n",
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    for ten_k in tqdm(ten_ks, desc='Lemmatize {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "        ten_k['file_lemma'] = lemmatize_words(word_pattern.findall(ten_k['file_clean']))\n",
    "        \n",
    "print_ten_k_data(ten_ks_by_ticker[example_ticker][:5], ['file_lemma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Remove Stop Words for INTC 10-Ks: 100%|██████████████████████████████████████████████| 25/25 [01:45<00:00,  4.21s/10-K]\n",
      "Remove Stop Words for GTLS 10-Ks: 100%|██████████████████████████████████████████████| 19/19 [00:12<00:00,  1.5110-K/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words Removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "lemma_english_stopwords = lemmatize_words(stopwords.words('english'))\n",
    "\n",
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    for ten_k in tqdm(ten_ks, desc='Remove Stop Words for {} 10-Ks'.format(ticker), unit='10-K'):\n",
    "        ten_k['file_lemma'] = [word for word in ten_k['file_lemma'] if word not in lemma_english_stopwords]\n",
    "        \n",
    "print('Stop Words Removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>uncertainty</th>\n",
       "      <th>litigious</th>\n",
       "      <th>constraining</th>\n",
       "      <th>interesting</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>abandon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>abandonment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>abandonments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>abdicate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>abdication</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    negative  positive  uncertainty  litigious  constraining  interesting  \\\n",
       "9       True     False        False      False         False        False   \n",
       "12      True     False        False      False         False        False   \n",
       "13      True     False        False      False         False        False   \n",
       "51      True     False        False      False         False        False   \n",
       "54      True     False        False      False         False        False   \n",
       "\n",
       "            word  \n",
       "9        abandon  \n",
       "12   abandonment  \n",
       "13  abandonments  \n",
       "51      abdicate  \n",
       "54    abdication  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments = ['negative', 'positive', 'uncertainty', 'litigious', 'constraining', 'interesting']\n",
    "\n",
    "sentiment_df = pd.read_csv('LoughranMcDonald_MasterDictionary_2018.csv')\n",
    "\n",
    "# Lowercase the columns for ease of use\n",
    "sentiment_df.columns = [column.lower() for column in sentiment_df.columns]\n",
    "\n",
    "# Remove unused information\n",
    "sentiment_df = sentiment_df[sentiments + ['word']]\n",
    "sentiment_df[sentiments] = sentiment_df[sentiments].astype(bool)\n",
    "sentiment_df = sentiment_df[(sentiment_df[sentiments]).any(1)]\n",
    "\n",
    "# Apply the same preprocessing to these words as the 10-k words\n",
    "sentiment_df['word'] = lemmatize_words(sentiment_df['word'].str.lower())\n",
    "sentiment_df = sentiment_df.drop_duplicates('word')\n",
    "\n",
    "\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    negative: '[[1 0 0 ... 0 1 0]\\n [1 0 0 ... 0 0 0]\\n [0 0 0 ....\n",
      "    positive: '[[17  0  0 ...  0  0  0]\\n [13  0  0 ...  0  1  0...\n",
      "    uncertainty: '[[ 0  0  0 ...  8  0 11]\\n [ 0  0  0 ...  6  0  9...\n",
      "    litigious: '[[0 0 0 ... 0 0 0]\\n [0 0 0 ... 0 0 0]\\n [0 0 0 ....\n",
      "    constraining: '[[0 8 0 ... 0 4 0]\\n [0 8 0 ... 0 3 0]\\n [0 0 0 ....\n",
      "    interesting: '[[1 1 0 ... 0 0 0]\\n [2 0 0 ... 0 0 0]\\n [0 0 0 ....},\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def get_bag_of_words(sentiment_words, docs):\n",
    "\n",
    "    vec = CountVectorizer(vocabulary=sentiment_words)\n",
    "    vectors = vec.fit_transform(docs)\n",
    "    words_list = vec.get_feature_names()\n",
    "    bag_of_words = np.zeros([len(docs), len(words_list)])\n",
    "    \n",
    "    for i in range(len(docs)):\n",
    "        bag_of_words[i] = vectors[i].toarray()[0]\n",
    "    return bag_of_words.astype(int)\n",
    "\n",
    "sentiment_bow_ten_ks = {}\n",
    "\n",
    "for ticker, ten_ks in ten_ks_by_ticker.items():\n",
    "    lemma_docs = [' '.join(ten_k['file_lemma']) for ten_k in ten_ks]\n",
    "    \n",
    "    sentiment_bow_ten_ks[ticker] = {\n",
    "        sentiment: get_bag_of_words(sentiment_df[sentiment_df[sentiment]]['word'], lemma_docs)\n",
    "        for sentiment in sentiments}\n",
    "    \n",
    "print_ten_k_data([sentiment_bow_ten_ks[example_ticker]], sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (24,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0ddd04b297d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         for sentiment_name, sentiment_values in ten_k_sentiments.items()}\n\u001b[0;32m     25\u001b[0m     for ticker, ten_k_sentiments in sentiment_bow_ten_ks.items()}\n\u001b[1;32m---> 26\u001b[1;33m plot_similarities(\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[0mjaccard_similarities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexample_ticker\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentiment\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentiment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentiments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mfile_dates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexample_ticker\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-163d1dea69ae>\u001b[0m in \u001b[0;36mplot_similarities\u001b[1;34m(similarities_list, dates, title, labels)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilarities_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrotation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2792\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2793\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2794\u001b[1;33m     return gca().plot(\n\u001b[0m\u001b[0;32m   2795\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m   2796\u001b[0m         is not None else {}), **kwargs)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m         \"\"\"\n\u001b[0;32m   1664\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1665\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1666\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyvizenv\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    270\u001b[0m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0;32m    271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (24,) and (1,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "\n",
    "def get_jaccard_similarity(bag_of_words_matrix):\n",
    "    \n",
    "    jaccard_similarities = []\n",
    "    bag_of_words_matrix = np.array(bag_of_words_matrix, dtype=bool)\n",
    "    \n",
    "    for i in range(len(bag_of_words_matrix)-1):\n",
    "            u = bag_of_words_matrix[i]\n",
    "            v = bag_of_words_matrix[i+1]\n",
    "              \n",
    "    jaccard_similarities.append(jaccard_score(u,v))    \n",
    "    \n",
    "    return jaccard_similarities\n",
    "\n",
    "# Get dates for the universe\n",
    "file_dates = {\n",
    "    ticker: [ten_k['file_date'] for ten_k in ten_ks]\n",
    "    for ticker, ten_ks in ten_ks_by_ticker.items()}\n",
    "jaccard_similarities = {\n",
    "    ticker: {\n",
    "        sentiment_name: get_jaccard_similarity(sentiment_values)\n",
    "        for sentiment_name, sentiment_values in ten_k_sentiments.items()}\n",
    "    for ticker, ten_k_sentiments in sentiment_bow_ten_ks.items()}\n",
    "plot_similarities(\n",
    "    [jaccard_similarities[example_ticker][sentiment] for sentiment in sentiments],\n",
    "    file_dates[example_ticker][1:],\n",
    "    'Jaccard Similarities for {} Sentiment'.format(example_ticker),\n",
    "    sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiment_tfidf_ten_ks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d3e580c3a133>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0msentiment_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mget_cosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentiment_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         for sentiment_name, sentiment_values in ten_k_sentiments.items()}\n\u001b[1;32m---> 17\u001b[1;33m     for ticker, ten_k_sentiments in sentiment_tfidf_ten_ks.items()}\n\u001b[0m\u001b[0;32m     18\u001b[0m project_helper.plot_similarities(\n\u001b[0;32m     19\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[0mcosine_similarities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexample_ticker\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentiment\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentiment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentiments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentiment_tfidf_ten_ks' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_cosine_similarity(tfidf_matrix):\n",
    "    \n",
    "    cosine_similarities = []\n",
    "    \n",
    "    for i in range(len(tfidf_matrix)-1):\n",
    "        \n",
    "        cosine_similarities.append(cosine_similarity(tfidf_matrix[i].reshape(1, -1),tfidf_matrix[i+1].reshape(1, -1))[0,0])\n",
    "    \n",
    "    return cosine_similarities\n",
    "\n",
    "cosine_similarities = {\n",
    "    ticker: {\n",
    "        sentiment_name: get_cosine_similarity(sentiment_values)\n",
    "        for sentiment_name, sentiment_values in ten_k_sentiments.items()}\n",
    "    for ticker, ten_k_sentiments in sentiment_tfidf_ten_ks.items()}\n",
    "project_helper.plot_similarities(\n",
    "    [cosine_similarities[example_ticker][sentiment] for sentiment in sentiments],\n",
    "    file_dates[example_ticker][1:],\n",
    "    'Cosine Similarities for {} Sentiment'.format(example_ticker),\n",
    "sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyvizenv]",
   "language": "python",
   "name": "conda-env-pyvizenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
